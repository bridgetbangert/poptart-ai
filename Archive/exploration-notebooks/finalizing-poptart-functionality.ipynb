{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10988486,"sourceType":"datasetVersion","datasetId":6837113},{"sourceId":10995868,"sourceType":"datasetVersion","datasetId":6844022}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom openai import OpenAI\nimport pandas as pd\nimport chardet\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:17:00.595362Z","iopub.execute_input":"2025-03-12T02:17:00.595751Z","iopub.status.idle":"2025-03-12T02:17:00.620605Z","shell.execute_reply.started":"2025-03-12T02:17:00.595722Z","shell.execute_reply":"2025-03-12T02:17:00.619430Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"os.environ['OPENAI_API_KEY'] = UserSecretsClient().get_secret(\"OPENAI_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:09:20.872138Z","iopub.execute_input":"2025-03-12T02:09:20.872634Z","iopub.status.idle":"2025-03-12T02:09:21.072899Z","shell.execute_reply.started":"2025-03-12T02:09:20.872584Z","shell.execute_reply":"2025-03-12T02:09:21.071816Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:13:14.312644Z","iopub.execute_input":"2025-03-12T03:13:14.313041Z","iopub.status.idle":"2025-03-12T03:13:14.381469Z","shell.execute_reply.started":"2025-03-12T03:13:14.313011Z","shell.execute_reply":"2025-03-12T03:13:14.380516Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"class XYZ():\n    \n    def __init__(self, job_desc):\n        self.job_desc = job_desc\n        self.prompt = {\n            \"role\": \"system\", \n            \"content\": f\"\"\"The XYZ formula is “Did [X] as measured by [Y] by doing [Z].”\n                If this bullet point is not in XYZ formatting, then rewrite the following bullet point to match the formula and highlight the technical work, challenges faced and overcame, the impact of their work.\n                If you do not have enough context, ask the user for some after giving suggestions to improve it. Whenever you are providing a number metric, replace it with 'XX' so that the user can fill it in.\n                The output should be 1-2 lines long with just one sentence and do not end it with a period.\n                Don't use personal pronouns.\n                Avoid using apostrophes ', ampersands &, and slashes /.\n                Avoid the excessive use of adjectives and adverbs.\n                Use digits instead of spelling out numbers.\n                Each bullet should begin with a strong, past-tense action verb: \n                Good examples of action verbs: analyzed, architected, automated, built, created, decreased, designed, developed, implemented, improved, optimized, published, reduced, refactored\n                Try your best to make it relevant to the following needs: {self.job_desc}.\"\"\"\n        }\n\n    def write_to_db(self):\n        # will do later when I have a database\n        pass\n\n\n    def input_bullet_point(self, bullet_point, model=\"gpt-4o-mini\"):\n        messages = [self.prompt, {\"role\": \"user\", \"content\": bullet_point}]\n        completion = client \\\n            .chat \\\n            .completions \\\n            .create(model=model, messages=messages)\n        \n        answer = completion.choices[0].message.content\n        self.write_to_db()\n        return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:13:18.044436Z","iopub.execute_input":"2025-03-12T03:13:18.044856Z","iopub.status.idle":"2025-03-12T03:13:18.052469Z","shell.execute_reply.started":"2025-03-12T03:13:18.044824Z","shell.execute_reply":"2025-03-12T03:13:18.050880Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"class Poptart(): # meow\n    \n    def __init__(self):\n        self.load_model_instructions()\n        self.initialize_model()\n\n    \n    def initialize_model(self):\n        self.messages = [\n            {\"role\": \"system\", \"content\": self.model_instructions},\n            {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n        ]\n        response = self.chat_with_model(self.messages)\n        self.messages.append({\"role\": \"assistant\", \"content\": response})\n        print(response)\n\n    \n    def load_model_instructions(self):\n        f = open(\"/kaggle/input/poptart-instructions/instructions.txt\", \"r\")\n        self.model_instructions = f.read()\n        f.close()\n\n\n    def chat_with_model(self, messages, model=\"gpt-4o\"):\n        completion = client \\\n            .chat \\\n            .completions \\\n            .create(model=model, messages=messages)\n        \n        return completion.choices[0].message.content\n\n    \n    def add_chat(self, input_text, model=\"gpt-4o-mini\"):\n        self.messages.append({\"role\": \"user\", \"content\": input_text})\n        response = self.chat_with_model(self.messages)\n        \n        self.messages.append({\"role\": \"assistant\", \"content\": response})\n        print(response)\n        return response\n\n    \n    def add_prompt_instructions(self, input_text):\n        self.messages.append({\"role\": \"system\", \"content\": input_text})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:13:35.298080Z","iopub.execute_input":"2025-03-12T03:13:35.298407Z","iopub.status.idle":"2025-03-12T03:13:35.306600Z","shell.execute_reply.started":"2025-03-12T03:13:35.298382Z","shell.execute_reply":"2025-03-12T03:13:35.305445Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"class Section():\n\n    def __init__(self, title):\n        self.title = title\n        self.dates = \"\"\n        self.content = []\n\n    def get_title(self):\n        return self.title\n\n    def get_dates(self):\n        return self.dates\n\n    def get_content(self):\n        return self.content\n\n    def set_dates(self, dates):\n        self.dates = dates\n\n    def add_content(self, item):\n        self.content.append(item)\n\n    def display_content(self):\n        [print(item) for item in self.content]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:25:48.954585Z","iopub.execute_input":"2025-03-12T03:25:48.954906Z","iopub.status.idle":"2025-03-12T03:25:48.961350Z","shell.execute_reply.started":"2025-03-12T03:25:48.954881Z","shell.execute_reply":"2025-03-12T03:25:48.959918Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"poptart = Poptart()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:13:40.259727Z","iopub.execute_input":"2025-03-12T03:13:40.260068Z","iopub.status.idle":"2025-03-12T03:13:44.173844Z","shell.execute_reply.started":"2025-03-12T03:13:40.260042Z","shell.execute_reply":"2025-03-12T03:13:44.172739Z"}},"outputs":[{"name":"stdout","text":"Well, Human, allow me to grace you with my presence. I am Poptart, a cat of refined taste and even more refined waistline. I'm here to sprinkle a bit of my exquisite expertise on the art of resume writing, particularly for tech professionals. My mom, a rather brilliant human, created this nifty app to aid in her job search, and I've decided to step down from my throne to assist you in yours.\n\nNow, since you're here seeking my guidance, why don't you tell me a bit about the job you're applying for? I suppose that's a start, even for someone who might not fully comprehend the intricacies of a properly prepared tuna. Oh, did I mention I'm a bit peckish? A lovely pate would certainly hit the spot.\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"job_desc = \"\"\"Minimum qualifications:\n\n    Master's degree in Statistics, Data Science, Mathematics, Physics, Economics, Operations Research, Engineering, or a related quantitative field.\n    8 years of work experience using analytics to solve product or business problems, coding (e.g., Python, R, SQL), querying databases or statistical analysis, or 6 years of work experience with a PhD degree.\n\n\nPreferred qualifications:\n\n    10 years of work experience using analytics to solve product or business problems, coding (e.g., Python, R, SQL), querying databases or statistical analysis, or 8 years of work experience with a PhD degree.\n\nAbout the job\n\nGoogle is and always will be an engineering company. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on millions, if not billions, of users. At Google, data scientists not only revolutionize search, they routinely work on massive scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, Social to Local, Google engineers are changing the world one technological achievement after another. As a Data Scientist, you will evaluate and improve Google's products. You will collaborate with a multi-disciplinary team of engineers and analysts on a wide range of problems. This position will bring scientific rigor and statistical methods to the challenges of product creation, development and improvement with an appreciation for the behaviors of the end user.\n\nThe US base salary range for this full-time position is $197,000-$291,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.\n\nPlease note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google.\nResponsibilities\n\n    Collaborate with stakeholders in cross-projects and team settings to identify and clarify business or product questions to answer. Provide feedback to translate and refine business questions into tractable analysis, evaluation metrics, or mathematical models.\n    Use custom data infrastructure or existing data models as appropriate, using specialized knowledge. Design and evaluate models to mathematically express and solve defined problems with limited precedent.\n    Gather information, business goals, priorities, and organizational context around the questions to answer, as well as the existing and upcoming data infrastructure.\n    Own the process of gathering, extracting, and compiling data across sources via relevant tools (e.g., SQL, R, Python). Independently format, re-structure, and/or validate data to ensure quality, and review the dataset to ensure it is ready for analysis.\"\"\"\n\npoptart.add_prompt_instructions(\"The user will then give you the job description or a job title. If the job description is given to you, you will search for main keywords and points that they would want to see on a resume. List them out.\")\njob_keys = poptart.add_chat(f\"Give me only the important information. No need for any other commentary, I am saving this content as a variable: {job_desc}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:13:44.175401Z","iopub.execute_input":"2025-03-12T03:13:44.175725Z","iopub.status.idle":"2025-03-12T03:13:48.726231Z","shell.execute_reply.started":"2025-03-12T03:13:44.175700Z","shell.execute_reply":"2025-03-12T03:13:48.725141Z"}},"outputs":[{"name":"stdout","text":"Minimum Qualifications:\n\n- Master's degree in Statistics, Data Science, Mathematics, Physics, Economics, Operations Research, Engineering, or related quantitative field\n- 8 years of experience using analytics for product or business problems, coding (e.g., Python, R, SQL), querying databases, or statistical analysis, or 6 years with a PhD\n\nPreferred Qualifications:\n\n- 10 years of experience using analytics for product or business problems, coding (e.g., Python, R, SQL), querying databases, or statistical analysis, or 8 years with a PhD\n\nResponsibilities:\n\n- Collaborate with stakeholders to clarify business or product questions and provide feedback to refine these into analysis, metrics, or models\n- Utilize custom or existing data models; design and evaluate models for solving problems\n- Gather business goals, priorities, and organizational context for questions, leveraging data infrastructure\n- Manage the data gathering, extraction, and compilation process using tools like SQL, R, and Python; ensure data quality and readiness for analysis\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"with open('/kaggle/input/resume-data/resume-data(ResumeContent).csv', 'rb') as f:\n    result = chardet.detect(f.read())\n\ndf = pd.read_csv('/kaggle/input/resume-data/resume-data(ResumeContent).csv', encoding=result['encoding'])[['section', 'content', 'org', 'dates']]\ndf.head()\n\nexperience_df = df[df['section'] == 'Experience']\nprojects_df = df[df['section'] == 'Projects']\neducation_df = df[df['section'] == 'Education']\nskills_df = df[df['section'] == 'Skills']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:13:50.642925Z","iopub.execute_input":"2025-03-12T03:13:50.643341Z","iopub.status.idle":"2025-03-12T03:13:50.774575Z","shell.execute_reply.started":"2025-03-12T03:13:50.643303Z","shell.execute_reply":"2025-03-12T03:13:50.773694Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"class ProjectsSection(Section):\n    def __init__(self):\n        super().__init__(\"Projects\")\n        \n    def add_content(self, item):\n        self.content.append(item)\n\nprojects_section = ProjectsSection()\nprojects_content = \"; \".join([f\"{project}\" for project in projects_df.content])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:26:03.259246Z","iopub.execute_input":"2025-03-12T03:26:03.259628Z","iopub.status.idle":"2025-03-12T03:26:03.265381Z","shell.execute_reply.started":"2025-03-12T03:26:03.259594Z","shell.execute_reply":"2025-03-12T03:26:03.264031Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:34:56.713055Z","iopub.execute_input":"2025-03-12T03:34:56.713397Z","iopub.status.idle":"2025-03-12T03:35:00.700622Z","shell.execute_reply.started":"2025-03-12T03:34:56.713369Z","shell.execute_reply":"2025-03-12T03:35:00.699539Z"}},"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"['Redesigned an undergraduate traffic prediction model as measured by accuracy and efficiency by implementing advanced algorithms and optimizing data processing techniques',\n 'Applied machine learning techniques to optimize traffic congestion forecasting as measured by improved prediction accuracy by developing and validating complex predictive models',\n 'Improved model generalization and reliability by integrating real-world data sources as measured by validation accuracy by optimizing data processing techniques',\n 'Leveraged Python and scikit-learn to improve model predictions by conducting time series analysis, enabling enhanced accuracy in forecasting outcomes',\n 'Implemented feature engineering techniques to extract relevant traffic-related signals as measured by improved model accuracy through the application of advanced statistical methods and data transformation techniques']"},"metadata":{}}],"execution_count":109},{"cell_type":"code","source":"xyz_helper = XYZ(job_desc)\npoptart.add_prompt_instructions(\"\"\"You will be writing the projects section of the resume. \n    This section is for personal projects, student design teams, and extracurricular/hobbyist projects, not projects from work. Do not include anything from the Experience section.\n    Don't use the word \"project\" in your project titles, it's redundant.\n    There's no need to disclose \"Personal Project\", \"Academic Project\", or \"Group Project\" beside your project title.\n    Each project should consist of multiple bullet points, not paragraphs.\n    Order projects and bullet points based on relevance to the specific job and general impressiveness. Put your best stuff first and grab the reader's attention!\"\"\")\n\nfor item in projects_df['org'].unique():\n    org_df = projects_df[projects_df['org'] == item]\n    section = Section(item)\n    dates = str(org_df['dates'].tolist()[0]).replace('nan', '')\n    \n    section.set_dates(dates)\n    \n    content = [[xyz_helper.input_bullet_point(x) for x in projects_df[projects_df['org'] == item]['content'].tolist()]]\n\n    poptart_response = poptart.add_chat(f\"Choose the most relevant points from this experience {content}, return as a string separated by new lines with the new guidelines. Keep the XYZ format.\").split('\\n')\n    response_clean = [xyz_helper.input_bullet_point(line.strip().lower()) for line in poptart_response]\n    section.add_content(response_clean)\n    projects_section.add_content(section)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:37:39.351447Z","iopub.execute_input":"2025-03-12T03:37:39.351835Z","iopub.status.idle":"2025-03-12T03:40:41.400142Z","shell.execute_reply.started":"2025-03-12T03:37:39.351788Z","shell.execute_reply":"2025-03-12T03:40:41.399149Z"}},"outputs":[{"name":"stdout","text":"Conducted statistical analysis on DNA methylation patterns, investigating potential epigenetic markers associated with cancer risk by using robust statistical methods  \nExplored large-scale genomic datasets, uncovering patterns in methylation variations to assess their potential impact on gene expression with statistical methods  \nCollaborated with a research team to analyze and interpret data, presenting findings that contributed to scientific discussions on the role of epigenetics in disease susceptibility using statistical methods  \nAnalyzed DNA methylation patterns in cancer patients by developing bespoke analytical frameworks, uncovering potential biomarkers and improving patient outcomes  \nIdentified biomarkers linked to cancer risk using R and statistical methods by overcoming data complexity and improving model accuracy  \nValidated findings by collaborating with a research partner, ensuring accurate analysis and interpretation through statistical rigor  \nDeveloped classification and risk assessment models by applying machine learning techniques, resulting in improved predictive accuracy and informed decision-making  \nCreated data visualization techniques, improving clarity of insights by presenting findings in a structured format\nConducted statistical analysis and machine learning modeling on 16 N3H datasets to identify high-risk populations vulnerable to Long COVID-19 by enhancing understanding of health disparities through rigorous data evaluation  \nDeveloped predictive models using XGBoost and PCA for dimensionality reduction, improving forecasting accuracy by automating hyperparameter tuning techniques to enhance model performance and precision  \nCollaborated with the research team to deliver actionable insights by synthesizing findings and translating them into strategic recommendations through comprehensive data analyses using advanced data science methodologies\nRedesigned an undergraduate traffic prediction model, improving accuracy metrics by applying advanced statistical methods and optimizing data processing techniques  \nOptimized forecasting of traffic congestion patterns, enhancing prediction accuracy by applying machine learning techniques and overcoming data quality challenges  \nIntegrated real-world data sources, improving model generalization and reliability by addressing data quality challenges and implementing robust data validation techniques  \nUtilized Python and scikit-learn to enhance model predictions through rigorous time series analysis, improving forecast accuracy  \nImplemented feature engineering techniques to extract relevant traffic-related signals, improving model accuracy by performing data transformation and analysis using Python and SQL\nConducted research to identify high-risk populations vulnerable to Long COVID-19 by analyzing relevant health data and demographic factors, employing predictive modeling techniques  \nAnalyzed health and genetic data from 16 N3H datasets by using statistical analysis and data cleaning techniques, improving dataset accuracy and informing health insights  \nImplemented predictive modeling using XGBoost by refining model parameters and addressing data imbalances, achieving improved accuracy in forecasting outcomes  \nExecuted principal component analysis to reduce dimensionality by improving data interpretability and performance through advanced statistical techniques, resulting in increased model efficiency  \nOptimized model performance and predictive accuracy by systematically experimenting with various configurations, tuning hyperparameters to achieve improved evaluation metrics  \nUtilized Python for data preprocessing and model training by leveraging pandas, NumPy, and scikit-learn, enhancing model performance and data quality  \nCollaborated with a research team to deliver actionable insights through data analysis and visualization techniques, increasing stakeholder engagement\nDesigned and enhanced a Kafka-based data ingestion framework to support 15+ source systems by implementing architectural improvements and optimization strategies, increasing data processing efficiency  \nImproved ETL performance and data reliability by implementing a metadata-driven development approach, reducing processing time  \nBuilt observability reports using SQL, Python, and HTML/CSS to monitor data pipeline health, ensuring timely identification of issues and optimizing data processing workflows  \nReduced data latency by optimizing Kafka ingestion and processing logic, implementing efficient coding strategies to improve data throughput  \nDelivered troubleshooting support for the Kafka framework by serving as the primary point of contact for technical issues, resulting in effective response time and resolution  \nDeveloped automation scripts to validate data quality and retrieve KPIs by leveraging programming in Python, reducing data processing time  \nIntegrated JDBC and Databricks REST API to automate data validation tasks, eliminating manual validation efforts and reducing processing time  \nImplemented automated checks to ensure accuracy and consistency in financial reporting, reducing errors in quarterly audits through thorough validation of data integrity  \nImproved reconciliation processes for finance teams by developing and implementing automated data validation tools, enhancing data accuracy\nDeveloped a Django-based OpenAI chatbot to analyze resumes using vector embeddings by implementing advanced natural language processing techniques, improving accuracy of resume evaluations and user engagement  \nCreated personalized resume feedback features by collaborating with stakeholders to identify user needs and leveraging data analysis, increasing user engagement metrics  \nConfigured application hosting on AWS instance under subdomain poptart-ai.bridgetbangert.com, overcoming infrastructure scalability challenges for successful deployment and uptime  \nTrained AI on specialized resume writing datasets by implementing iterative model adjustments, enhancing recommendation accuracy and user guidance  \nImproved login functionality and templating by optimizing user authentication processes, resulting in enhanced personalization features and increased user engagement  \nPlanned future iterations to enhance response engagement by integrating sass and improving efficiency in user information storage through advanced data structuring techniques\n","output_type":"stream"}],"execution_count":111}]}